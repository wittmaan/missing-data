---
title: "Missing Data"
subtitle: "and how to deal with them..."
author: "Andreas Wittmann"
date: "2021/05/14 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false

---

class: center, middle

# Machine Learning development from <br> model-centric to data-centric

Andrew Ng: “When a system isn’t performing well, many teams instinctually try to improve the code. But for many practical applications, it’s more effective instead to focus on improving the data,”

.footnote[
https://www.deeplearning.ai/the-batch/issue-84/
]

---

## model-centric view

- Collect what data you can, and develop a model good enough to deal with the noise
in the data.

- Hold the data fixed and iteratively improve the code/model.

## data-centric view
- The consistency of the data is paramount. Use tools to improve the data quality
$\rightarrow$ this will allow multiple models to do well.

- Hold the model fixed and iteratively improve the data.


.footnote[
https://www.deeplearning.ai/the-batch/issue-84/
]

---

# From big data to good data 

MLOps' most important taks: ensure consistently high-quality data in all phases of 
the ML project lifecycle.

**Good data is:**

- Defined consistently (definition of labels y is unambiguous )
- Cover of important cases (good coverage of inputs x)
- Has timely feedback from production data (distribution covers data drift and concept drift)
- Sized appropriately

.footnote[
https://www.deeplearning.ai/the-batch/issue-84/
]


---

# Good data without missing data

- Getting high-quality data also includes tackling noise data

- Data can become noise caused of missings

## Missing data

- Industrial data can have different challanges
  - privacy
  - expense
  - machine or human mistakes

(describe when missing data can happen and how)

---

# The problem

- Many AI/ML/Data science methods are developed for complete data

- Inappropriate approach imposes noise or bias on data

---

class: inverse, center, middle

# Types of missingness

---

# Missing completely at random (MCAR)

- no particular reason the data is missing

- no relationship / dependency between missingness and observed values 
  - men or women are not more inclined to share their salary info
  - gender does not impact on salary

---

# Missing at random (MAR)

- there is a relationship / dependency between missing values and observed ones, but not the missing values

---

# Missing not at random (MNAR)

- the cause of missingness it not known and we cannot draw any conclusion from observed data


---

# Hints

- Distinguishing the type is missingness is not easy, sometimes it's impossible

- The size and balance of data must be considered before distinguising the type

- Rule of thumb: assume missing at random unless there is a good reason not to!



---


class: inverse, center, middle

# How to deal with missingness

---

## Dropping (ignoring) missing values

- Listwise deletion: cases with missing data are just dropped
  - sample size
  - loss of information
  - induced error of imputation

## Imputation techniques

- Single imputation

- Multiple imputation through support vector machines

- Multiple imputation by chained equations

- Data augmentation


- Multiple imputation

---

- Discriminative models

$$X_{\text{miss}} = f(X_{\text{obs}}) (\text{MAR})$$

assume corelation between variables with missing values and the observed variables

$\rightarrow$ find corelation by estimating a function

- Generative models

$$P(X_{\text{miss}})(\text{MCAR})$$
(estimate normal distribution)

and

$$P(X_{\text{miss}} \mid X_{\text{obs}})(\text{MAR})$$

(estimate conditional distribution)

---
# Single imputation

- Mean substitution (generative)
- K-nearest neighbor (KNN) (discriminative)
- Discriminative model training 
  - linear regression
  - neural nets
  - random forest


$\rightarrow$ don't account for uncertainty

---

# Multiple Imputation

- accounts for uncertainty by creating multiple imputed version of data

- Bootstrapping (subselection of the data, do the imputation, ...)

- Generative models (draw samples from the estimated distribution)

- MICE (multivariate imputation by chained equations)

  - starts with initial imputation e.g. mean imputation
  - at each cycle only one variable is considered missing and is imputed via other variables
  - the whole process may be repeated

- denoising autoencoder

---


# Thank you! Questions?



# Literature

- youtube: Reza Sahraeian - The industrial challenge of missing data | PyData Eindhoven 2020
