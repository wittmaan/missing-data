---
title: "Missing Data"
subtitle: "and how to deal with them..."
author: "Andreas Wittmann"
date: "2021/05/21 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default-fonts, default, slides.css]
    nature:
      highlightStyle: github
      highlightLines: true
      highlightLanguage: ["r", "css", "yaml"]
      countIncrementalSlides: false

      

---

```{r, echo=FALSE, message=FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE, bib.style = "authoryear", style = "markdown", dashed = TRUE)

bib <- ReadBib("slides.bib")
ui <- "- "
```

```{r setup, echo=FALSE, message=FALSE, include=FALSE}
knitr::opts_chunk$set(fig.path="figs/", echo=FALSE, warning=FALSE, message=FALSE, fig.retina=3, fig.asp=.5, out.width='100%', fig.showtext = TRUE, comment = NULL)

require(ggplot2)
require(gridExtra)
require(viridis)
```



class: center, middle
background-color: #7899d4

# Machine Learning development from <br> model-centric to data-centric

Andrew Ng: “When a system isn’t performing well, many teams instinctually try to improve the code. But for many practical applications, it’s more effective instead to focus on improving the data,”

.footnote[
https://www.deeplearning.ai/the-batch/issue-84/
]

---

## model-centric view

- Collect what data you can, and develop a model good enough to deal with the noise
in the data.

- Hold the data fixed and iteratively improve the code/model.

## data-centric view
- The consistency of the data is paramount. Use tools to improve the data quality
$\rightarrow$ this will allow multiple models to do well.

- Hold the model fixed and iteratively improve the data.


.footnote[
https://www.deeplearning.ai/the-batch/issue-84/
]

---

# From big data to good data 

MLOps' most important taks: ensure consistently high-quality data in all phases of 
the ML project lifecycle.

**Good data is:**

- Defined consistently (definition of labels y is unambiguous )
- Cover of important cases (good coverage of inputs x)
- Has timely feedback from production data (distribution covers data drift and concept drift)
- Sized appropriately

.footnote[
https://www.deeplearning.ai/the-batch/issue-84/
]


---

# Good data without missing data

- Getting high-quality data also includes tackling noise data

- Data can become noise caused of missings

- Using only the complete cases for the analysis can lead to a dramatic information loss

# Missing data

**Missing data is everwhere** sooner or later anyone who does statistics will encounter missing data

Can arise for many reasons:
  - Non-Response e.g. in surveys
  - Lost data due to machine or human mistakes
  - Bug issues in non-mandatory fields
  - Privacy
  - ...

---

# The problem

- Many AI/ML/Data science methods are developed for complete data

- Inappropriate approach imposes noise or bias on data

- Missing data can lead to the risk of incorrect conclusions due to absence of relevant information leading to invalid results

- The quality of statistical analysis can be only good as the quality of the data

---

# Terminology

- **full data** $Z=(Z_{\text{obs}}, Z_{\text{mis}})$
- **observed data** $Z_{\text{obs}}$ 
- **missing data** $Z_{\text{mis}}$

- Given $n \times p$ data matrix $Z$, which can contain missing data

- $Z = (Y, X)$, i.e. $Y$ matrix dependent and $X$ matrix independent variables

- indicator matrix $R$ build from $Z$ as

$$R_{ij} = \left\{\begin{array}{cl}
              1 & \textrm{if } Z_{ij} \textrm{ obs} \\
              0 & \textrm{if } Z_{ij} \textrm{ mis} \\
            \end{array} \right. \quad \text{for} \ i=1,\ldots,n \ \text{and} \ j=1,\ldots,p.$$

---

class: center, middle
background-color: #7899d4

# Types of missingness

---

# Missing completely at random (MCAR)

Probability of missingness is completely independent from observed and unobserved/missing values

$$P(r_i \mid z_i) = P(r_i \mid z_{i,\,\textrm{obs}}, z_{i,\,\textrm{mis}}) = P(r_i), \quad \text{for} \ i=1,\ldots,n,$$

where $z_{i,\,\textrm{obs}}$ represents the observed and $z_{i,\,\textrm{mis}}$ the missing values from the $i$-th row $z_i$ of
the data matrix $Z$.

- No particular reason that the data is missing

- Often an unrealistic assumption

- **Example:** Weighing scale that ran out of batteries 

---

# Missing at random (MAR)

The probability for missigness of values is only dependent of the 
observed values $z_{i,\,\textrm{obs}}$ 

$$P(r_i \mid z_i) = P(r_i \mid z_{i,\,\textrm{obs}}, z_{i,\,\textrm{mis}}) = P(r_i \mid z_{i,\,\textrm{obs}}), \quad \text{for} \ i=1,\ldots,n.$$

- Relationship between missing values and observed ones

- More realistic than MCAR 

- Modern missing data methods generally start from the MAR assumption

- **Example:** Weighing scale may produce more missing data when placed on a soft surface and type of surface is known 

---

# Missing not at random (MNAR)

The probability for the missigness of values is dependent of the observed $z_{i,\,\textrm{obs}}$ and unobserved values $z_{i,\,\textrm{mis}}$

$$P(r_i \mid z_i) = P(r_i \mid z_{i,\,\textrm{obs}}, z_{i,\,\textrm{mis}}), \quad \text{for} \ i=1,\ldots,n.$$

- Cause of missingness it not known and we cannot draw any conclusion from observed data

- **Example:** Weighing scale mechanism may wear out over time, but time is not part of the dataset


---

# Best practices (`r Citet(bib, "Buuren2018")`)

- Distinguishing the type is missingness is not easy, sometimes it's impossible

- The size and balance of data must be considered before distinguising the type

- Under MCAR, one can analyze the observed observation and ignore discard any missing observations

- Rule of thumb: assume missing at random unless there is a good reason not to!

---


class: center, middle
background-color: #7899d4

# Look at the data

---

## airquality dataset

- Daily air quality measurements in New York, May to September 1973.

- Daily readings of the following air quality values for May 1, 1973 (a Tuesday) to September 30, 1973.

  - **Ozone:** Mean ozone in parts per billion from 1300 to 1500 hours at Roosevelt Island

  - **Solar.R:** Solar radiation in Langleys in the frequency band 4000–7700 Angstroms from 0800 to 1200 hours at Central Park

  - **Wind:** Average wind speed in miles per hour at 0700 and 1000 hours at LaGuardia Airport

  - **Temp:** Maximum daily temperature in degrees Fahrenheit at La Guardia Airport.
  
Source: The data were obtained from the New York State Department of Conservation (ozone data) and the National Weather Service (meteorological data).

---

## airquality dataset

```{r, echo=FALSE, warning=FALSE, message=FALSE}
require(knitr)
require(VIM)
require(mice)
kable(airquality[1:10,], format = "html")
```


---

## missing data pattern

```{r, echo=FALSE, warning=FALSE, message=FALSE}
p <- md.pattern(airquality)
```

---

## missing value frequency

```{r, echo=FALSE, warning=FALSE, message=FALSE, include=FALSE}
require(DataExplorer)
p <- plot_missing(airquality) +   
  theme_minimal() +
  scale_fill_viridis_d() 
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
p
```

---

## marginplot

```{r, echo=FALSE, warning=FALSE, message=FALSE}
require(VIM)
marginplot(airquality[, c("Solar.R", "Ozone")])
```

- blue one summarizes the distribution of Ozone when Solar.R is missing,  red one summarizes it when it is unobserved

- distribution of observed data given the other variable is observed
for MCAR, blue and red box plots should be similar

<!--
- if MCAR is violated, complete case analysis will be biased
-->

---

class: center, middle
background-color: #7899d4

# How to deal with missingness

---

## Missing values methods

- Dropping missing values

- Imputation techniques

  - Single imputation

  - Multiple imputation


---

## Dropping (ignoring) missing values

### Listwise deletion
  
- Cases with missing values are excluded from the data set. 

- Only the complete cases are analyzed or used.  

- Loss of information dependent on the fraction of missing data
  
- Often the default way of handling incomplete data 

- If the data are MCAR, listwise deletion produces unbiased estimates of means, variances and regression weights.

- `r Citet(bib, "Schafer2002")`: *If a missing data problem can be resolved by discarding only a small part of the sample, then the method can be quite effective.*




---

## Mean/Median imputation

- Missing values are replaced by 

  - the mean value for quantitative variables 
  
  - by the most frequently occurring category for qualitative variables
  
- Imputed value is an estimate, thus there is uncertainty about its true value

- Uncertainty is measued by its standard error

- Too small standard errors 

---


## Mean/Median imputation


```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
require(mice)
require(dplyr)
require(reshape2)

imp <- mice(airquality, 
                   method = "mean", # Replace by mean of the other values
                   m = 1, # Number of multiple imputations. 
                   maxit = 1,
                  print=FALSE
            ) # Number of iteration; mostly useful for  convergence

data <- complete(imp)
#airquality.ext <- data.frame(airquality, Ozone.imp=data[,"Ozone"]) %>% #melt(measure.vars=c("Ozone", "Ozone.imp"), variable.name="type")
airquality.ext <- airquality %>%
  mutate(
    type=ifelse(is.na(Ozone), "Ozone.imp", "Ozone"),
    Ozone.imp=ifelse(is.na(Ozone), data[,"Ozone"], Ozone)
    )
```



```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
p1 <- airquality.ext %>% 
  ggplot(aes(x=Ozone.imp, fill=type)) +  
  geom_histogram(position="dodge") +
  theme_minimal() +
  scale_fill_viridis_d()

p2 <- airquality.ext %>%
  ggplot(aes(y=Ozone.imp, x=Solar.R, colour=type)) +
  geom_point() +
  theme_minimal() +
  scale_color_viridis_d() 

grid.arrange(p1, p2, ncol=2)
```

---

## Regression Imputation

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
fit <- lm(Ozone ~ Solar.R, data = airquality.ext)
pred <- predict(fit, newdata = data.frame(Solar.R = airquality.ext$Solar.R))

# imp2 <- mice(airquality, 
#                    method = "norm.predict", # Replace by mean of the other values
#                    m = 1, # Number of multiple imputations. 
#                    maxit = 1,
#                   print=FALSE
#             ) # Number of iteration; mostly useful for  convergence
# 
# data2 <- complete(imp2)

airquality.ext2 <- airquality %>%
  mutate(
    type=ifelse(is.na(Ozone), "Ozone.imp", "Ozone"),
    Ozone.imp=ifelse(is.na(Ozone), pred, Ozone)
    )
```

- Regression imputation incorporates knowledge of other variables with the idea of producing smarter imputations. 

- The first step involves building a model from the observed data. 

- Predictions for the incomplete cases are then calculated under the fitted model, and serve as replacements for the missing data.

```{r}
require(equatiomatic)

extract_eq(fit, use_coefs = FALSE)
```


---

## Regression Imputation


```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
require(ggplot2)
require(gridExtra)
require(viridis)

p1 <- airquality.ext2 %>% 
  ggplot(aes(x=Ozone.imp, fill=type)) +  
  geom_histogram(position="dodge") +
  theme_minimal() +
  scale_fill_viridis_d()

p2 <- airquality.ext2 %>%
  ggplot(aes(y=Ozone.imp, x=Solar.R, colour=type)) +
  geom_point() +
  theme_minimal() +
  scale_color_viridis_d() 

grid.arrange(p1, p2, ncol=2)
```

---

## Stochastic Regression Imputation

- One disadvantage to regression imputation is that it used the fitted model without error terms. Therefore, the imputed results are too close to the regression line and biased the correlations, reduced the variance of the data.

- Stochastic regression corrected this problem by adding a error term when imputing the values, which potentially better reflect the correlations between variables


---

## Stochastic Regression Imputation

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
imputation3 <- mice(airquality, method = "norm.nob", # stochastic regression imputation
                    m = 1, maxit = 1,
                    seed = 1, print = FALSE)

data3 <- complete(imputation3)

airquality.ext8 <- airquality %>%
  mutate(
    type=ifelse(is.na(Ozone), "Ozone.imp", "Ozone"),
    Ozone.imp=ifelse(is.na(Ozone), data3[,"Ozone"], Ozone)
    )

p1 <- airquality.ext8 %>% 
  ggplot(aes(x=Ozone.imp, fill=type)) +  
  geom_histogram(position="dodge") +
  theme_minimal() +
  scale_fill_viridis_d()

p2 <- airquality.ext8 %>%
  ggplot(aes(y=Ozone.imp, x=Solar.R, colour=type)) +
  geom_point() +
  theme_minimal() +
  scale_color_viridis_d() 

grid.arrange(p1, p2, ncol=2)
```


---


## Multiple imputation 

- Missing values are replaced by chained regression, where $m$ complete datasets are generated (`r Citet(bib, "Raghunathan2001")`)

- Bootstrapping (subselection of the data, do the imputation, ...)

- accounts for uncertainty by creating multiple imputed version of data

- Generative models (draw samples from the estimated distribution)

- MICE (multivariate imputation by chained equations)

<!--
  - starts with initial imputation e.g. mean imputation
  - at each cycle only one variable is considered missing and is imputed via other variables
  - the whole process may be repeated

In multiple imputation data analysis, three steps will be taken:

- Step 0: Set imputation model
- Step 1: The incomplete dataset will be imputed $m$ times;
- Step 2: Each complete dataset will be analyzed separately by proper standard regression model;
- Step 3: The analysis results will be pooled together by Rubin’s rules

-->

---


## Multiple imputation

![](figs/multiple_imputation.png)


source: https://stefvanbuuren.name/fimd/sec-nutshell.html

---

## Multiple imputation

Accounts for incertainty with the following four-step approach:

1. Create $m$ complete versions of the data by replacing missing values by plausible ones with a random component

1. The $m$ imputed datasets are 
  - identical for the observed data entries
  - differ in the imputed values
  
  *The magnitude of these difference reflects uncertainty about what value to impute*
  
1. Analyze each of the $m$ complete datasets. Each set of parameter estimates differs slightly because of the random component

1. Pool the $m$ parameter estimates into one estimate. 
   Variance combines 
   - the conventional sampling variance (within-imputation variance)
   - extra variance caused by the missing data (between-imputation variance).

<!--
Under the appropriate conditions, the pooled estimates are unbiased and have the correct statistical properties.
-->

---

## Multiple imputation

### How large should $m$ be (`r Citet(bib, "Buuren2018")`)?

Classic advice: $m=3, 5, 10$. More recently: set $m$ higher: 20 to 100.

Some advice:

- Use $m=5$ or $m=10$ if the fraction of missing information is low

- Develop your model with $m=5$. Do final run with $m$ equal to percentage of incomplete cases

---

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
imp <- mice(airquality, seed=1, m=5, print=FALSE)
```


## Multiple imputation

```{r}
densityplot(imp, thicker = 4)
```


---

## Multiple imputation

```{r}
#fit <- with(imp, lm(Ozone ~ Solar.R))
#summary(pool(fit))


bwplot(imp)
```

---

## Multiple imputation

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
require(ggplot2)
require(gridExtra)
require(viridis)

data3 <- complete(imp, 1)

airquality.ext4 <- airquality %>%
  mutate(
    type=ifelse(is.na(Ozone), "Ozone.imp", "Ozone"),
    Ozone.imp=ifelse(is.na(Ozone), data3[,"Ozone"], Ozone)
    )

p1 <- airquality.ext4 %>% 
  ggplot(aes(x=Ozone.imp, fill=type)) +  
  geom_histogram(position="dodge") +
  theme_minimal() +
  scale_fill_viridis_d()

p2 <- airquality.ext4 %>%
  ggplot(aes(y=Ozone.imp, x=Solar.R, colour=type)) +
  geom_point() +
  theme_minimal() +
  scale_color_viridis_d() 

grid.arrange(p1, p2, ncol=2)
```

---

# Multiple imputation in detail...


---

# Takeaways

- understand the missing type and data before anything (tips: missing rate, balance, correlation, data size, ...)

- There is no single magical method to deal with missingness, the right choice depends on your data

- Benefit from multiple imputation to account for uncertainty

- Be vigilant in using open source packages

- Check literature for new methodologies

---

class: center, middle
background-color: #7899d4

# Thank you! Questions?

---

# Literature

<!--
```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE}
writeLines(ui)
print(bib[key="Richman2007"], 
      .opts = list(check.entries = FALSE, 
                   style = "html", 
                   bib.style = "authoryear"))
```
-->
 
```{r, results='asis', echo=FALSE}
PrintBibliography(bib)
```

---

# Literature

- youtube: Reza Sahraeian - The industrial challenge of missing data | PyData Eindhoven 2020

- https://bookdown.org/mwheymans/bookmi/

- https://stefvanbuuren.name/publication/vanbuuren-2018/

- http://pol346.com/2021/week10_02.html#1

- https://htmlpreview.github.io/?https://raw.githubusercontent.com/ehsanx/spph504-007/master/Lab6/lab6part1.html

- https://rstudio-pubs-static.s3.amazonaws.com/445649_5f323f9cc6aa4333b404882e67e9c344.html

- https://biostat.wisc.edu/~lmao/missing_data/Chap%201.%20Introduction.pdf

- https://arxiv.org/abs/1805.07405

- https://qdata.github.io/deep2Read//deep2reproduce/2019Fall//T28_SuYiwenys5kh_missingDataByNN.pdf

- https://www.youtube.com/watch?v=dIiGW2vvCF0

- https://arxiv.org/abs/1806.02920

- https://hal.inria.fr/hal-03044144/file/how_to_deal_with_missing_data_in_supervised_deep_learning_.pdf
