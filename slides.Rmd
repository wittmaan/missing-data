---
title: "Missing Data"
subtitle: "and how to deal with them..."
author: "Andreas Wittmann"
date: "2021/06/11  (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default-fonts, default, slides.css]
    nature:
      highlightStyle: github
      highlightLines: true
      highlightLanguage: ["r", "css", "yaml"]
      countIncrementalSlides: false

      

---

```{r, echo=FALSE, message=FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE, bib.style = "authoryear", style = "markdown", dashed = TRUE)

bib <- ReadBib("slides.bib")
ui <- "- "
```

```{r setup, echo=FALSE, message=FALSE, include=FALSE}
knitr::opts_chunk$set(fig.path="figs/", echo=FALSE, warning=FALSE, message=FALSE, fig.retina=3, fig.asp=.5, out.width='100%', fig.showtext = TRUE, comment = NULL)

require(ggplot2)
require(gridExtra)
require(viridis)
require(randomForest)
```



class: center, middle
background-color: #7899d4

# Machine Learning development from <br> Model-Centric to Data-Centric

Andrew Ng: 

> When a system isn’t performing well, many teams instinctually try to improve the code.<br> But for many practical applications, it’s more effective instead to focus on improving the data,

.footnote[
https://www.deeplearning.ai/the-batch/issue-84/
]

---

## Model-Centric view

- Collect what data you can

- Develop a model good enough to deal with the noise in the data

- Hold the data fixed and iteratively improve the code/model

## Data-Centric view

- The consistency of the data is paramount

- Use tools to improve the data quality

- Hold the model fixed and iteratively improve the data


.footnote[
https://www.deeplearning.ai/the-batch/issue-84/
]

???

Data-Centric view $\rightarrow$ *This will allow multiple models to do well*

---

# From big data to good data 

MLOps' most important task: 

> ensure consistently high-quality data in all phases of the ML project lifecycle

## Good data is

- Defined consistently

- Cover of important cases

- Has timely feedback from production data

- Sized appropriately

.footnote[
https://www.deeplearning.ai/the-batch/issue-84/
]

???

- Defined consistently (definition of labels y is unambiguous)

- Cover of important cases (good coverage of inputs x)

- Has timely feedback from production data (distribution covers data drift and concept drift)

---

# Good data without missing data

- Getting high-quality data also includes tackling noise data

- Data can become noise caused of missings

# Missing data

Can arise for many reasons:
  - Non-Response e.g. in surveys
  - Lost data due to machine or human mistakes
  - Bug issues in non-mandatory fields
  - join, merge
  - Different variable per source
  - Different number of categories per source
  - ...

???

**Missing data is everwhere** sooner or later anyone who does statistics will encounter missing data

---

# The problem

```{r, echo=TRUE, eval=FALSE}
x <- c(10, NA, 20, 30, 40, 20)

mean(x)
```

--

```{r}
x <- c(10, NA, 20, 30, 40, 20)
```


```{r}
mean(x)
```

--

```{r, echo=TRUE, eval=FALSE}
randomForest(Ozone ~ . , data=airquality)
```

--

```{r, error=TRUE}
randomForest(Ozone ~ . , data=airquality)
```

---

# The problem

- Many AI/ML/Data science methods are developed for complete data

- Using only the complete cases for the analysis can lead to dramatic information loss

- Inappropriate approach imposes noise or bias on data

- Can lead to incorrect conclusions due to absense of relevant information

- The quality of statistical analysis can be only as good as the quality of the data

---

# Terminology

- **Full / complete data** $Z=(Z^{\textrm{obs}}, Z^{\textrm{mis}})$

- **Observed / incomplete data** $Z^{\textrm{obs}}$ 

- **Unobserved / missing data** $Z^{\textrm{mis}}$

- **Complete cases** subset of rows without missing values

- Given $n \times p$ data matrix $Z$, which can contain missing data

- $Z = (Y, X)$, i.e. $Y$ matrix dependent and $X$ matrix independent variables

- Indicator matrix $R$ build from $Z$ as

$$R_{ij} = \left\{\begin{array}{cl}
              1 & \textrm{if } Z_{ij} \textrm{ obs} \\
              0 & \textrm{if } Z_{ij} \textrm{ mis} \\
            \end{array} \right. \quad \text{for} \ i=1,\ldots,n \ \text{and} \ j=1,\ldots,p.$$

---

class: center, middle
background-color: #7899d4

# Types of missingness

---

# Missing completely at random (MCAR)

Probability of missingness is completely independent from observed and unobserved/missing values:

$$P(r_i \mid z_i) = P(r_i \mid z_{i}^\textrm{obs}, z_{i}^\textrm{mis}) = P(r_i), \quad \text{for} \ i=1,\ldots,n,$$

$z_{i}^\textrm{obs}$ observed, $z_{i}^\textrm{mis}$ missing values from the $i$-th row $z_i$ of the data matrix $Z$

- No particular reason that the data is missing

- Often an unrealistic assumption

- **Example:** Weighing scale that ran out of batteries 

---

# Missing at random (MAR)

Probability for missigness of values is only dependent of the observed values $z_{i}^\textrm{obs}$ 

$$P(r_i \mid z_i) = P(r_i \mid z_{i}^\textrm{obs}, z_{i}^\textrm{mis}) = P(r_i \mid z_{i}^\textrm{obs}), \quad \text{for} \ i=1,\ldots,n.$$

- More realistic than MCAR 

- Modern missing data methods generally start from the MAR assumption

- **Example:** Weighing scale may produce more missing data when placed on a soft surface and type of surface is known 

---

# Missing not at random (MNAR)

Probability for missigness of values is dependent of the observed $z_{i}^\textrm{obs}$ and unobserved values $z_{i}^\textrm{mis}$

$$P(r_i \mid z_i) = P(r_i \mid z_{i}^\textrm{obs}, z_{i}^\textrm{mis}), \quad \text{for} \ i=1,\ldots,n.$$

- Cause of missingness it not known 

- We cannot draw any conclusion from observed data

- **Example:** Weighing scale mechanism may wear out over time, but time is not part of the dataset


---

class: center, middle
background-color: #7899d4

# How to deal with missingness

---

## Strategies to deal with missing data

- Prevention - impossible for ex-post analyses

- Dropping missing values

- Imputation techniques

  - Single imputation

  - Multiple imputation


---

class: center, middle
background-color: #7899d4

# Look at the data

---

## Airquality Dataset

- Daily air quality measurements in New York, May to September 1973.

- Daily readings of the following air quality values for May 1, 1973 (a Tuesday) to September 30, 1973.

  - **Ozone:** Mean ozone in parts per billion from 1300 to 1500 hours at Roosevelt Island

  - **Solar.R:** Solar radiation in Langleys in the frequency band 4000–7700 Angstroms from 0800 to 1200 hours at Central Park

  - **Wind:** Average wind speed in miles per hour at 0700 and 1000 hours at LaGuardia Airport

  - **Temp:** Maximum daily temperature in degrees Fahrenheit at La Guardia Airport.
  
Source: The data were obtained from the New York State Department of Conservation (ozone data) and the National Weather Service (meteorological data).

---

## Airquality Dataset

```{r, echo=FALSE, warning=FALSE, message=FALSE}
require(knitr)
require(VIM)
require(mice)
kable(airquality[1:10,], format = "html")
```


---

## Missing data pattern

```{r, echo=FALSE, warning=FALSE, message=FALSE}
p <- md.pattern(airquality)
```

---

## Missing value frequency

```{r, echo=FALSE, warning=FALSE, message=FALSE, include=FALSE}
require(DataExplorer)
p <- plot_missing(airquality) +   
  theme_minimal() +
  scale_fill_viridis_d() 
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
p
```

---

## Marginplot

```{r, echo=FALSE, warning=FALSE, message=FALSE}
require(VIM)
marginplot(airquality[, c("Solar.R", "Ozone")], col = viridis(2))
```

???

- Violet box plot summarizes the distribution of Ozone when Solar.R is observed, yellow one summarizes it when it is unobserved

- Distribution of observed data given the other variable is observed
for MCAR, violet and yellow box plots should be similar

---

## Dropping (ignoring) missing values

### Listwise deletion
  
- Only the complete cases are analyzed 

- Advantages:
  - Simple - Often the default way of handling incomplete data 
  
  - Under MCAR: unbiased estimates of means, variances and regression weights 

  - `r Citet(bib, "Schafer2002")`: *If a missing data problem can be resolved by discarding only a small part of the sample, then the method can be quite effective.*

- Disadvantages:
  - Loss of information dependent on the fraction of missing data
  
  - Larger standard errors
  
  - Under MAR: biased, even for simple statistics like the mean

---

## Mean/Median imputation

- Missing values are replaced by 

  - The mean value for quantitative variables 
  
  - The most frequently occurring category for qualitative variables
  
- Imputed value is an estimate, thus there is uncertainty about its true value

- Uncertainty is measued by its standard error

- Too small standard errors 

---


## Mean/Median imputation


```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
require(mice)
require(dplyr)
require(reshape2)

imp <- mice(airquality, method = "mean",  m = 1, maxit = 1, print=FALSE) 

data <- complete(imp)
airquality.imp.mean <- airquality %>%
  mutate(
    type=ifelse(is.na(Ozone), "imp", "obs"),
    Ozone.imp=ifelse(is.na(Ozone), data[,"Ozone"], Ozone)
    )
```



```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
p1 <- airquality.imp.mean %>% 
  ggplot(aes(x=Ozone.imp, fill=type)) +  
  geom_histogram(position="dodge") +
  xlab("Ozone") +
  theme_minimal() +
  scale_fill_viridis_d(direction=-1)

p2 <- airquality.imp.mean %>%
  ggplot(aes(y=Ozone.imp, x=Solar.R, colour=type)) +
  geom_point() +
  ylab("Ozone") +
  theme_minimal() +
  scale_color_viridis_d(direction=-1) 

grid.arrange(p1, p2, ncol=2)
```

---

## Regression Imputation

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
fit <- lm(Ozone ~ Solar.R, data = airquality.imp.mean)
pred <- predict(fit, newdata = data.frame(Solar.R = airquality.imp.mean$Solar.R))

airquality.imp.reg <- airquality %>%
  mutate(
    type=ifelse(is.na(Ozone), "imp", "obs"),
    Ozone.imp=ifelse(is.na(Ozone), pred, Ozone)
    )
```

- Regression imputation incorporates knowledge of other variables

- The first step involves building a model from the observed data 

- Calculate predictions for the incomplete cases under the fitted model

```{r}
require(equatiomatic)
extract_eq(fit, use_coefs = FALSE)
```


---

## Regression Imputation


```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
require(ggplot2)
require(gridExtra)
require(viridis)

p1 <- airquality.imp.reg %>% 
  ggplot(aes(x=Ozone.imp, fill=type)) +  
  geom_histogram(position="dodge") +
  xlab("Ozone") +
  theme_minimal() +
  scale_fill_viridis_d(direction=-1)

p2 <- airquality.imp.reg %>%
  ggplot(aes(y=Ozone.imp, x=Solar.R, colour=type)) +
  geom_point() +
  ylab("Ozone") +
  theme_minimal() +
  scale_color_viridis_d(direction=-1) 

grid.arrange(p1, p2, ncol=2)
```

???

- Almost no variation in the imputed values

---

## Stochastic Regression Imputation

- Regression imputation disadvantage: 

  - Fitted model is used without error terms

  - Imputed results are too close to the regression line

  - Biased correlations, reduced the variance of the data

- Stochastic regression adds an error term when imputing the values

???

- Stochastic regression adds an error term when imputing the values

  $\rightarrow$ *Potentially better reflects the correlations between variables*

---

## Stochastic Regression Imputation

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
imp.sto.reg <- mice(airquality, method = "norm.nob", m = 1, maxit = 1, seed = 1, print = FALSE)

data3 <- complete(imp.sto.reg)

airquality.imp.storeg <- airquality %>%
  mutate(
    type=ifelse(is.na(Ozone), "imp", "obs"),
    Ozone.imp=ifelse(is.na(Ozone), data3[,"Ozone"], Ozone)
    )

p1 <- airquality.imp.storeg %>% 
  ggplot(aes(x=Ozone.imp, fill=type)) +  
  geom_histogram(position="dodge") +
  xlab("Ozone") +
  theme_minimal() +
  scale_fill_viridis_d(direction=-1)

p2 <- airquality.imp.storeg %>%
  ggplot(aes(y=Ozone.imp, x=Solar.R, colour=type)) +
  geom_point() +
  ylab("Ozone") +
  theme_minimal() +
  scale_color_viridis_d(direction=-1) 

grid.arrange(p1, p2, ncol=2)
```

???

- Negative values imputed

---


## Multiple imputation 

- Missing values are replaced by chained regression, where $m$ complete datasets are generated (`r Citet(bib, "Raghunathan2001")`)

- Bootstrapping (subselection of the data, do the imputation, ...)

- Accounts for uncertainty by creating multiple imputed version of data

- Generative models (draw samples from the estimated distribution)

- MICE (multivariate imputation by chained equations)

<!--
  - starts with initial imputation e.g. mean imputation
  - at each cycle only one variable is considered missing and is imputed via other variables
  - the whole process may be repeated

In multiple imputation data analysis, three steps will be taken:

- Step 0: Set imputation model
- Step 1: The incomplete dataset will be imputed $m$ times;
- Step 2: Each complete dataset will be analyzed separately by proper standard regression model;
- Step 3: The analysis results will be pooled together by Rubin’s rules

-->

---


## Multiple imputation

![](figs/multiple_imputation.png)


Source: https://stefvanbuuren.name/fimd/sec-nutshell.html

---

## Multiple imputation (`r Citet(bib, "VanBuuren2018")`)

1. Specify an imputation model $P(Y_j^\mathrm{mis}|Y_j^\mathrm{obs}, Y_{-j}, R)$ for variable $Y_j$ with $j=1,\dots,p$.

2. For each $j$, fill in starting imputations $\dot Y_j^0$ by random draws from $Y_j^\mathrm{obs}$.

3. Repeat for $t = 1,\dots,m$.

4. Repeat for $j = 1,\dots,p$.

5. Define $\dot Y_{-j}^t = (\dot Y_1^t,\dots,\dot Y_{j-1}^t,\dot Y_{j+1}^{t-1},\dots,\dot Y_p^{t-1})$ as the currently complete
    data except $Y_j$.
    
6. Draw $\dot\phi_j^t \sim P(\phi_j^t|Y_j^\mathrm{obs}, \dot Y_{-j}^t, R)$.

7. Draw imputations $\dot Y_j^t \sim P(Y_j^\mathrm{mis}|Y_j^\mathrm{obs}, \dot Y_{-j}^t, R, \dot\phi_j^t)$.
   
8. End repeat $j$.

9. End repeat $t$.

---

## Multiple imputation

1. Create $m$ complete versions of the data by replacing missing values by plausible ones with a random component (steps 1 to 3)

1. The $m$ imputed datasets are 
  - identical for the observed data entries
  - differ in the imputed values
  
1. Analyze each of the $m$ complete datasets. Each set of parameter estimates differs slightly because of the random component

1. Pool the $m$ parameter estimates into one estimate. 
   Variance combines 
   - the conventional sampling variance (within-imputation variance)
   - extra variance caused by the missing data (between-imputation variance).

???

1. The $m$ imputed datasets are 
  - identical for the observed data entries
  - differ in the imputed values
  
  *The magnitude of these difference reflects uncertainty about what value to impute*

---

## Multiple imputation

### How large should $m$ be (`r Citet(bib, "VanBuuren2018")`)?

Classic advice: $m=3, 5, 10$. More recently: set $m$ higher: 20 to 100.

Some advice:

- Use $m=5$ or $m=10$ if the fraction of missing information is low

- Develop your model with $m=5$. Do final run with $m$ equal to percentage of incomplete cases

---

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
imp <- mice(airquality, seed=1, m=5, maxit = 50, print=FALSE)
```


## Multiple imputation

```{r}
densityplot(imp, thicker = 4)
```


---

## Multiple imputation

```{r}
#fit <- with(imp, lm(Ozone ~ Solar.R))
#summary(pool(fit))


bwplot(imp)
```

---

## Multiple imputation

```{r}
plot(imp)
```

???

One convergence, the different streams should be freely intermingled with each other, without showing any definite trends (`r Citet(bib, "VanBuuren2011")`)

---

## Multiple imputation

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
require(ggplot2)
require(gridExtra)
require(viridis)

data3 <- complete(imp, 1)

airquality.ext4 <- airquality %>%
  mutate(
    type=ifelse(is.na(Ozone), "Ozone.imp", "Ozone"),
    Ozone.imp=ifelse(is.na(Ozone), data3[,"Ozone"], Ozone)
    )

p1 <- airquality.ext4 %>% 
  ggplot(aes(x=Ozone.imp, fill=type)) +  
  geom_histogram(position="dodge") +
  theme_minimal() +
  scale_fill_viridis_d()

p2 <- airquality.ext4 %>%
  ggplot(aes(y=Ozone.imp, x=Solar.R, colour=type)) +
  geom_point() +
  theme_minimal() +
  scale_color_viridis_d() 

grid.arrange(p1, p2, ncol=2)
```

---

# Multiple imputation in detail...

## 1. Start

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
dat.tmp <- airquality[10:12,]

require(kableExtra)
dat.tmp %>% 
  kbl(row.names = FALSE) %>% 
  kable_minimal() %>%
  column_spec(1, color = c("red", "black", "black")) %>%
  column_spec(2, color = c("black", "red", "black"))
```

--

## 2. Mean Imputation

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
dat.tmp.imp <- dat.tmp
dat.tmp.imp$Ozone[1] <- mean(dat.tmp.imp$Ozone, na.rm = T)
dat.tmp.imp$Solar.R[2] <- mean(dat.tmp.imp$Solar.R, na.rm = T)

dat.tmp.imp %>% 
  kbl(row.names = FALSE) %>% 
  kable_minimal() %>%
  column_spec(1, color = c("red", "black", "black")) %>%
  column_spec(2, color = c("black", "red", "black")) 
```

---

# Multiple imputation in detail...

## 3. Set Ozone to NA's / Regression on complete cases

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
dat.tmp.imp$Ozone[1] <- NA

dat.tmp.imp %>% 
  kbl(row.names = FALSE) %>% 
  kable_minimal() %>%
  column_spec(1, color = c("red", "black", "black")) %>%
  column_spec(2, color = c("black", "red", "black")) %>%
  row_spec(2:3, color="white", "background" = "green")
```

--

## 4. Predict Ozone 

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
dat.tmp.imp$Ozone[1] <- 12.51

dat.tmp.imp %>% 
  kbl(row.names = FALSE) %>% 
  kable_minimal() %>%
  column_spec(1, color = c("green", "black", "black")) %>%
  column_spec(2, color = c("black", "red", "black")) 
```

---

# Multiple imputation in detail...

## 5. Set Solar.R to NA's / Regression on complete cases

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
dat.tmp.imp$Solar.R[2] <- NA

dat.tmp.imp %>% 
  kbl(row.names = FALSE) %>%
  kable_minimal() %>%
  column_spec(2, color = c("black", "red", "black")) %>%
  row_spec(c(1,3), color="white", "background" = "green")
```

--

## 6. Predict Solar.R 

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
dat.tmp.imp$Solar.R[2] <- 201.41

dat.tmp.imp %>% 
  kbl(row.names = FALSE) %>% 
  kable_minimal() %>%
  column_spec(1, color = c("green", "black", "black")) %>%
  column_spec(2, color = c("black", "green", "black")) 
```


---

# Multiple imputation in detail...

## 7. Set Ozone to NA's / Regression on complete cases

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
dat.tmp.imp$Ozone[1] <- NA

dat.tmp.imp %>% 
  kbl(row.names = FALSE) %>% 
  kable_minimal() %>%
  column_spec(1, color = c("red", "black", "black")) %>%
  column_spec(2, color = c("black", "red", "black")) %>%
  row_spec(2:3, color="white", "background" = "green")
```

--

## Repeat until convergence

---

# Software (R)

### mice

Multiple imputation using Fully Conditional Specification (FCS) implemented by the MICE algorithm as described in Van Buuren and Groothuis-Oudshoorn (2011)

### VIM 

New tools for the visualization of missing and/or imputed values are introduced, which can be used for exploring the data and the structure of the missing and/or imputed values.

### Amelia

Implements Bootstrap multiple imputation using EM to estimate the parameters, for quantitative data it imputes assuming a Multivariate Gaussian distribution.

---

# Software (Python)

### sklearn.impute

- SimpleImputer: Imputation transformer for completing missing values.
- IterativeImputer: Multivariate imputer that estimates each feature from all the others.
- KNNImputer: Imputation for completing missing values using k-Nearest Neighbors.

### missingno

Small toolset of flexible and easy-to-use missing data visualizations and utilities that allows you to get a quick visual summary of the completeness (or lack thereof) of your dataset.

### fancyimpute

A variety of matrix completion and imputation algorithms (including MICE) implemented in Python 3.6.

---

# Best practices (`r Citet(bib, "VanBuuren2018")`)

- Distinguishing the type of missingness is not easy, sometimes it's impossible

- The size and balance of data must be considered before distinguising the type

- Under MCAR, one can analyze the observed observation and ignore discard any missing observations

- **Rule of thumb:** Assume MAR unless there is a good reason not to!

---

# Takeaways

- Understand the missing type and data before anything (tips: missing rate, balance, correlation, data size, ...)

- There is no single magical method to deal with missingness, the right choice depends on your data

- Benefit from multiple imputation to account for uncertainty

- Be vigilant in using open source packages

- Check literature for new methodologies

---


background-color: #7899d4

# Thank you! Questions?

<br><br><br><br>

### Slides: https://github.com/wittmaan/missing-data

---

# Literature

<!--
```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE}
writeLines(ui)
print(bib[key="Richman2007"], 
      .opts = list(check.entries = FALSE, 
                   style = "html", 
                   bib.style = "authoryear"))
```
-->
 
```{r, results='asis', echo=FALSE}
PrintBibliography(bib)
```

---

# Links

- https://www.deeplearning.ai/the-batch/issue-84/

- https://stefvanbuuren.name/publication/vanbuuren-2018/

- http://pol346.com/2021/week10_02.html#1

- https://htmlpreview.github.io/?https://raw.githubusercontent.com/ehsanx/spph504-007/master/Lab6/lab6part1.html

- https://rstudio-pubs-static.s3.amazonaws.com/445649_5f323f9cc6aa4333b404882e67e9c344.html

- https://s3.amazonaws.com/assets.datacamp.com/production/course_17404/slides/chapter4.pdf


