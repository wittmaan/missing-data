---
title: "Missing Data"
subtitle: "and how to deal with them..."
author: "Andreas Wittmann"
date: "2021/05/14 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false

---

```{r, echo=FALSE, message=FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE, bib.style = "authoryear", style = "markdown", dashed = TRUE)

bib <- ReadBib("slides.bib")
ui <- "- "
```


class: center, middle

# Machine Learning development from <br> model-centric to data-centric

Andrew Ng: “When a system isn’t performing well, many teams instinctually try to improve the code. But for many practical applications, it’s more effective instead to focus on improving the data,”

.footnote[
https://www.deeplearning.ai/the-batch/issue-84/
]

---

## model-centric view

- Collect what data you can, and develop a model good enough to deal with the noise
in the data.

- Hold the data fixed and iteratively improve the code/model.

## data-centric view
- The consistency of the data is paramount. Use tools to improve the data quality
$\rightarrow$ this will allow multiple models to do well.

- Hold the model fixed and iteratively improve the data.


.footnote[
https://www.deeplearning.ai/the-batch/issue-84/
]

---

# From big data to good data 

MLOps' most important taks: ensure consistently high-quality data in all phases of 
the ML project lifecycle.

**Good data is:**

- Defined consistently (definition of labels y is unambiguous )
- Cover of important cases (good coverage of inputs x)
- Has timely feedback from production data (distribution covers data drift and concept drift)
- Sized appropriately

.footnote[
https://www.deeplearning.ai/the-batch/issue-84/
]


---

# Good data without missing data

- Getting high-quality data also includes tackling noise data

- Data can become noise caused of missings

- Using only the complete cases for the analysis can lead to a dramatic information loss

# Missing data

Can arise for many reasons:
  - Non-Response e.g. in surveys
  - Lost data due to machine or human mistakes
  - Bug issues in non-mandatory fields
  - Privacy
  - ...

---

# The problem

- Many AI/ML/Data science methods are developed for complete data

- Inappropriate approach imposes noise or bias on data

- Missing data can lead to the risk of incorrect conclusions due to absence of relevant information leading to invalid results

- The quality of statistical analysis can be only good as the quality of the data

---

# Terminology

- **full data** $Z=(Z_{\text{obs}}, Z_{\text{mis}})$
- **observed data** $Z_{\text{obs}}$ 
- **missing data** $Z_{\text{mis}}$

- Given $n \times p$ data matrix $Z$, which can contain missing data

- $Z = (Y, X)$, i.e. $Y$ matrix dependent and $X$ matrix independent variables

- indicator matrix $R$ build from $Z$ as

$$R_{ij} = \left\{\begin{array}{cl}
              1 & \textrm{if } Z_{ij} \textrm{ obs} \\
              0 & \textrm{if } Z_{ij} \textrm{ mis} \\
            \end{array} \right. \quad \text{for} \ i=1,\ldots,n \ \text{and} \ j=1,\ldots,p.$$

---

class: inverse, center, middle

# Types of missingness

---

# Missing completely at random (MCAR)

Probability of missingness is completely independent from observed and unobserved/missing values

$$P(r_i \mid z_i) = P(r_i \mid z_{i,\,\textrm{obs}}, z_{i,\,\textrm{mis}}) = P(r_i), \quad \text{for} \ i=1,\ldots,n,$$

where $z_{i,\,\textrm{obs}}$ represents the observed and $z_{i,\,\textrm{mis}}$ the missing values from the $i$-th row $z_i$ of
the data matrix $Z$.

- No particular reason that the data is missing

- Often an unrealistic assumption

- **Example:** Weighing scale that ran out of batteries 

---

# Missing at random (MAR)

The probability for missigness of values is only dependent of the 
observed values $z_{i,\,\textrm{obs}}$ 

$$P(r_i \mid z_i) = P(r_i \mid z_{i,\,\textrm{obs}}, z_{i,\,\textrm{mis}}) = P(r_i \mid z_{i,\,\textrm{obs}}), \quad \text{for} \ i=1,\ldots,n.$$

- Relationship between missing values and observed ones

- More realistic than MCAR 

- Modern missing data methods generally start from the MAR assumption

- **Example:** Weighing scale may produce more missing data when placed on a soft surface and type of surface is known 

---

# Missing not at random (MNAR)

The probability for the missigness of values is dependent of the observed $z_{i,\,\textrm{obs}}$ and unobserved values $z_{i,\,\textrm{mis}}$

$$P(r_i \mid z_i) = P(r_i \mid z_{i,\,\textrm{obs}}, z_{i,\,\textrm{mis}}), \quad \text{for} \ i=1,\ldots,n.$$

- Cause of missingness it not known and we cannot draw any conclusion from observed data

- **Example:** Weighing scale mechanism may wear out over time, but time is not part of the dataset


---

# Best practices (`r Citet(bib, "Buuren2018")`)

- Distinguishing the type is missingness is not easy, sometimes it's impossible

- The size and balance of data must be considered before distinguising the type

- Under MCAR, one can analyze the observed observation and ignore discard any missing observations

- Rule of thumb: assume missing at random unless there is a good reason not to!

---


class: inverse, center, middle

# Look at the data

---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
require(VIM)
require(mice)
data(nhanes)
nhanes
```

---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
md.pattern(nhanes)
```

---

class: inverse, center, middle

# How to deal with missingness

---

## Dropping (ignoring) missing values

### Listwise deletion
  
- Cases with missing values are excluded from the data set. 

- Only the complete cases are analyzed or used.  

- Loss of information dependent on the fraction of missing data
  
---

## Imputation techniques

### Mean/Median imputation

Missing values are replaced by the mean value for quantitative variables and by the most frequently occurring category for qualitative variables

### Multiple imputation through support vector machines (`r Citet(bib, "Richman2007")`)

Missing values are replaced by repeated application of a support vector regression

---

## Imputation techniques

### Multiple imputation by chained equations (`r Citet(bib, "Raghunathan2001")`)

Missing values are replaced by chained regression, where e.g. five complete datasets are generated

### Data augmentation  (`r Citet(bib, "Schafer1997")`)

Use Monte Carlo Methods to generate e.g. five complete datasets

---

- Discriminative models

$$X_{\text{miss}} = f(X_{\text{obs}}) (\text{MAR})$$

assume corelation between variables with missing values and the observed variables

$\rightarrow$ find corelation by estimating a function

- Generative models

$$P(X_{\text{miss}})(\text{MCAR})$$
(estimate normal distribution)

and

$$P(X_{\text{miss}} \mid X_{\text{obs}})(\text{MAR})$$

(estimate conditional distribution)

---
# Single imputation

- Mean substitution (generative)
- K-nearest neighbor (KNN) (discriminative)
- Discriminative model training 
  - linear regression
  - neural nets
  - random forest


$\rightarrow$ don't account for uncertainty

---

# Multiple Imputation

- accounts for uncertainty by creating multiple imputed version of data

- Bootstrapping (subselection of the data, do the imputation, ...)

- Generative models (draw samples from the estimated distribution)

- MICE (multivariate imputation by chained equations)

  - starts with initial imputation e.g. mean imputation
  - at each cycle only one variable is considered missing and is imputed via other variables
  - the whole process may be repeated

- denoising autoencoder

---

# Takeaways

- understand the missing type and data before anything (tips: missing rate, balance, correlation, data size, ...)

- There is no single magical method to deal with missingness, the right choice depends on your data

- Benefit from multiple imputation to account for uncertainty

- Be vigilant in using open source packages

- Check literature for new methodologies

---


# Thank you! Questions?



# Literature

<!--
```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE}
writeLines(ui)
print(bib[key="Richman2007"], 
      .opts = list(check.entries = FALSE, 
                   style = "html", 
                   bib.style = "authoryear"))
```
-->
 
```{r, results='asis', echo=FALSE}
PrintBibliography(bib)
```

- youtube: Reza Sahraeian - The industrial challenge of missing data | PyData Eindhoven 2020

- https://bookdown.org/mwheymans/bookmi/

- https://stefvanbuuren.name/publication/vanbuuren-2018/

- https://htmlpreview.github.io/?https://raw.githubusercontent.com/ehsanx/spph504-007/master/Lab6/lab6part1.html

- https://rstudio-pubs-static.s3.amazonaws.com/445649_5f323f9cc6aa4333b404882e67e9c344.html

- https://biostat.wisc.edu/~lmao/missing_data/Chap%201.%20Introduction.pdf

- https://arxiv.org/abs/1805.07405

- https://qdata.github.io/deep2Read//deep2reproduce/2019Fall//T28_SuYiwenys5kh_missingDataByNN.pdf

- https://www.youtube.com/watch?v=dIiGW2vvCF0

- https://arxiv.org/abs/1806.02920

- https://hal.inria.fr/hal-03044144/file/how_to_deal_with_missing_data_in_supervised_deep_learning_.pdf
