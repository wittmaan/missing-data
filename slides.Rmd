---
title: "Missing Data"
subtitle: "and how to deal with them..."
author: "Andreas Wittmann"
date: "2021/05/14 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default-fonts, default, slides.css]
    nature:
      highlightStyle: github
      highlightLines: true
      highlightLanguage: ["r", "css", "yaml"]
      countIncrementalSlides: false
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
      ratio: "16:9"

---

```{r, echo=FALSE, message=FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE, bib.style = "authoryear", style = "markdown", dashed = TRUE)

bib <- ReadBib("slides.bib")
ui <- "- "
```

```{r setup, echo=FALSE, message=FALSE, include=FALSE}
knitr::opts_chunk$set(fig.path="figs/", echo=FALSE, warning=FALSE, message=FALSE, fig.retina=3, fig.asp=.5, out.width='100%', fig.showtext = TRUE, comment = NULL)
```



class: center, middle
background-color: #7899d4

# Machine Learning development from <br> model-centric to data-centric

Andrew Ng: “When a system isn’t performing well, many teams instinctually try to improve the code. But for many practical applications, it’s more effective instead to focus on improving the data,”

.footnote[
https://www.deeplearning.ai/the-batch/issue-84/
]

---

## model-centric view

- Collect what data you can, and develop a model good enough to deal with the noise
in the data.

- Hold the data fixed and iteratively improve the code/model.

## data-centric view
- The consistency of the data is paramount. Use tools to improve the data quality
$\rightarrow$ this will allow multiple models to do well.

- Hold the model fixed and iteratively improve the data.


.footnote[
https://www.deeplearning.ai/the-batch/issue-84/
]

---

# From big data to good data 

MLOps' most important taks: ensure consistently high-quality data in all phases of 
the ML project lifecycle.

**Good data is:**

- Defined consistently (definition of labels y is unambiguous )
- Cover of important cases (good coverage of inputs x)
- Has timely feedback from production data (distribution covers data drift and concept drift)
- Sized appropriately

.footnote[
https://www.deeplearning.ai/the-batch/issue-84/
]


---

# Good data without missing data

- Getting high-quality data also includes tackling noise data

- Data can become noise caused of missings

- Using only the complete cases for the analysis can lead to a dramatic information loss

# Missing data

Can arise for many reasons:
  - Non-Response e.g. in surveys
  - Lost data due to machine or human mistakes
  - Bug issues in non-mandatory fields
  - Privacy
  - ...

---

# The problem

- Many AI/ML/Data science methods are developed for complete data

- Inappropriate approach imposes noise or bias on data

- Missing data can lead to the risk of incorrect conclusions due to absence of relevant information leading to invalid results

- The quality of statistical analysis can be only good as the quality of the data

---

# Terminology

- **full data** $Z=(Z_{\text{obs}}, Z_{\text{mis}})$
- **observed data** $Z_{\text{obs}}$ 
- **missing data** $Z_{\text{mis}}$

- Given $n \times p$ data matrix $Z$, which can contain missing data

- $Z = (Y, X)$, i.e. $Y$ matrix dependent and $X$ matrix independent variables

- indicator matrix $R$ build from $Z$ as

$$R_{ij} = \left\{\begin{array}{cl}
              1 & \textrm{if } Z_{ij} \textrm{ obs} \\
              0 & \textrm{if } Z_{ij} \textrm{ mis} \\
            \end{array} \right. \quad \text{for} \ i=1,\ldots,n \ \text{and} \ j=1,\ldots,p.$$

---

class: inverse, center, middle
background-color: #7899d4

# Types of missingness

---

# Missing completely at random (MCAR)

Probability of missingness is completely independent from observed and unobserved/missing values

$$P(r_i \mid z_i) = P(r_i \mid z_{i,\,\textrm{obs}}, z_{i,\,\textrm{mis}}) = P(r_i), \quad \text{for} \ i=1,\ldots,n,$$

where $z_{i,\,\textrm{obs}}$ represents the observed and $z_{i,\,\textrm{mis}}$ the missing values from the $i$-th row $z_i$ of
the data matrix $Z$.

- No particular reason that the data is missing

- Often an unrealistic assumption

- **Example:** Weighing scale that ran out of batteries 

---

# Missing at random (MAR)

The probability for missigness of values is only dependent of the 
observed values $z_{i,\,\textrm{obs}}$ 

$$P(r_i \mid z_i) = P(r_i \mid z_{i,\,\textrm{obs}}, z_{i,\,\textrm{mis}}) = P(r_i \mid z_{i,\,\textrm{obs}}), \quad \text{for} \ i=1,\ldots,n.$$

- Relationship between missing values and observed ones

- More realistic than MCAR 

- Modern missing data methods generally start from the MAR assumption

- **Example:** Weighing scale may produce more missing data when placed on a soft surface and type of surface is known 

---

# Missing not at random (MNAR)

The probability for the missigness of values is dependent of the observed $z_{i,\,\textrm{obs}}$ and unobserved values $z_{i,\,\textrm{mis}}$

$$P(r_i \mid z_i) = P(r_i \mid z_{i,\,\textrm{obs}}, z_{i,\,\textrm{mis}}), \quad \text{for} \ i=1,\ldots,n.$$

- Cause of missingness it not known and we cannot draw any conclusion from observed data

- **Example:** Weighing scale mechanism may wear out over time, but time is not part of the dataset


---

# Best practices (`r Citet(bib, "Buuren2018")`)

- Distinguishing the type is missingness is not easy, sometimes it's impossible

- The size and balance of data must be considered before distinguising the type

- Under MCAR, one can analyze the observed observation and ignore discard any missing observations

- Rule of thumb: assume missing at random unless there is a good reason not to!

---


class: inverse, center, middle
background-color: #7899d4

# Look at the data

---

## A small data set with non-monotone missing values.

.pull-left[
- age: Age group (1=20-39, 2=40-59, 3=60+)

- bmi: Body mass index (kg/m**2)

- hyp: Hypertensive (1=no,2=yes)

- chl: Total serum cholesterol (mg/dL)
]

.pull-right[
```{r, echo=FALSE, warning=FALSE, message=FALSE}
require(knitr)
require(VIM)
require(mice)
data(nhanes)
kable(nhanes[1:10,], format = "html")
```
]


---

.pull-left[
```{r, echo=FALSE, warning=FALSE, message=FALSE}
tmp <- md.pattern(nhanes)
```
]

.pull-right[
```{r, echo=FALSE, warning=FALSE, message=FALSE}
tmp
```


]

---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
require(DataExplorer)
plot_missing(nhanes)
```


---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
require(VIM)
marginplot(nhanes[, c("hyp", "bmi")])
```

---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
require(VIM)
marginplot(nhanes[, c("hyp", "chl")])
```

---


```{r, echo=FALSE, warning=FALSE, message=FALSE}
require(VIM)
marginplot(nhanes[, c("chl", "bmi")])
```

---

distribution of observed data given the other variable is observed
for MCAR, blue and red box plots should be similar

---

class: inverse, center, middle
background-color: #7899d4

# How to deal with missingness

---

## Dropping (ignoring) missing values

### Listwise deletion
  
- Cases with missing values are excluded from the data set. 

- Only the complete cases are analyzed or used.  

- Loss of information dependent on the fraction of missing data
  
- Often the default way of handling incomplete data 

- If the data are MCAR, listwise deletion produces unbiased estimates of means, variances and regression weights.

- `r Citet(bib, "Schafer2002")`: *If a missing data problem can be resolved by discarding only a small part of the sample, then the method can be quite effective.*

---

## Imputation techniques

### Mean/Median imputation

Missing values are replaced by the mean value for quantitative variables and by the most frequently occurring category for qualitative variables

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
require(mice)
require(dplyr)
require(reshape2)

imp <- mice(airquality, 
                   method = "mean", # Replace by mean of the other values
                   m = 1, # Number of multiple imputations. 
                   maxit = 1,
                  print=FALSE
            ) # Number of iteration; mostly useful for  convergence

data <- complete(imp)
#airquality.ext <- data.frame(airquality, Ozone.imp=data[,"Ozone"]) %>% #melt(measure.vars=c("Ozone", "Ozone.imp"), variable.name="type")
airquality.ext <- airquality %>%
  mutate(
    type=ifelse(is.na(Ozone), "Ozone.imp", "Ozone"),
    Ozone.imp=ifelse(is.na(Ozone), data[,"Ozone"], Ozone)
    )
```

---


### Mean/Median imputation



```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
require(ggplot2)
require(gridExtra)
require(viridis)

p1 <- airquality.ext %>% 
  ggplot(aes(x=Ozone.imp, fill=type)) +  
  geom_histogram(position="dodge") +
  theme_minimal() +
  scale_fill_viridis_d()

p2 <- airquality.ext %>%
  ggplot(aes(y=Ozone.imp, x=Solar.R, colour=type)) +
  geom_point() +
  theme_minimal() +
  scale_color_viridis_d() 

grid.arrange(p1, p2, ncol=2)
```


---

## Imputation techniques

### Regression Imputation

- Regression imputation incorporates knowledge of other variables with the idea of producing smarter imputations. 

- The first step involves building a model from the observed data. 

- Predictions for the incomplete cases are then calculated under the fitted model, and serve as replacements for the missing data.

---

### Regression Imputation

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
fit <- lm(Ozone ~ Solar.R, data = airquality.ext)
pred <- predict(fit, newdata = data.frame(Solar.R = airquality.ext$Solar.R))

airquality.ext2 <- airquality %>%
  mutate(
    type=ifelse(is.na(Ozone), "Ozone.imp", "Ozone"),
    Ozone.imp=ifelse(is.na(Ozone), pred, Ozone)
    )
```


```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
require(ggplot2)
require(gridExtra)
require(viridis)

p1 <- airquality.ext2 %>% 
  ggplot(aes(x=Ozone.imp, fill=type)) +  
  geom_histogram(position="dodge") +
  theme_minimal() +
  scale_fill_viridis_d()

p2 <- airquality.ext2 %>%
  ggplot(aes(y=Ozone.imp, x=Solar.R, colour=type)) +
  geom_point() +
  theme_minimal() +
  scale_color_viridis_d() 

grid.arrange(p1, p2, ncol=2)
```

---

### Multiple imputation through support vector machines (`r Citet(bib, "Richman2007")`)

Missing values are replaced by repeated application of a support vector regression

---

## Imputation techniques

### Multiple imputation by chained equations (`r Citet(bib, "Raghunathan2001")`)

Missing values are replaced by chained regression, where e.g. five complete datasets are generated

### Data augmentation  (`r Citet(bib, "Schafer1997")`)

Use Monte Carlo Methods to generate e.g. five complete datasets

---

- Discriminative models

$$X_{\text{miss}} = f(X_{\text{obs}}) (\text{MAR})$$

assume corelation between variables with missing values and the observed variables

$\rightarrow$ find corelation by estimating a function

- Generative models

$$P(X_{\text{miss}})(\text{MCAR})$$
(estimate normal distribution)

and

$$P(X_{\text{miss}} \mid X_{\text{obs}})(\text{MAR})$$

(estimate conditional distribution)

---
# Single imputation

- Mean substitution (generative)
- K-nearest neighbor (KNN) (discriminative)
- Discriminative model training 
  - linear regression
  - neural nets
  - random forest


$\rightarrow$ don't account for uncertainty

---

# Multiple Imputation

- accounts for uncertainty by creating multiple imputed version of data

- Bootstrapping (subselection of the data, do the imputation, ...)

- Generative models (draw samples from the estimated distribution)

- MICE (multivariate imputation by chained equations)

  - starts with initial imputation e.g. mean imputation
  - at each cycle only one variable is considered missing and is imputed via other variables
  - the whole process may be repeated

- denoising autoencoder

---

# Takeaways

- understand the missing type and data before anything (tips: missing rate, balance, correlation, data size, ...)

- There is no single magical method to deal with missingness, the right choice depends on your data

- Benefit from multiple imputation to account for uncertainty

- Be vigilant in using open source packages

- Check literature for new methodologies

---


# Thank you! Questions?



# Literature

<!--
```{r, results='asis', echo=FALSE, warning=FALSE, message=FALSE}
writeLines(ui)
print(bib[key="Richman2007"], 
      .opts = list(check.entries = FALSE, 
                   style = "html", 
                   bib.style = "authoryear"))
```
-->
 
```{r, results='asis', echo=FALSE}
PrintBibliography(bib)
```

- youtube: Reza Sahraeian - The industrial challenge of missing data | PyData Eindhoven 2020

- https://bookdown.org/mwheymans/bookmi/

- https://stefvanbuuren.name/publication/vanbuuren-2018/

- https://htmlpreview.github.io/?https://raw.githubusercontent.com/ehsanx/spph504-007/master/Lab6/lab6part1.html

- https://rstudio-pubs-static.s3.amazonaws.com/445649_5f323f9cc6aa4333b404882e67e9c344.html

- https://biostat.wisc.edu/~lmao/missing_data/Chap%201.%20Introduction.pdf

- https://arxiv.org/abs/1805.07405

- https://qdata.github.io/deep2Read//deep2reproduce/2019Fall//T28_SuYiwenys5kh_missingDataByNN.pdf

- https://www.youtube.com/watch?v=dIiGW2vvCF0

- https://arxiv.org/abs/1806.02920

- https://hal.inria.fr/hal-03044144/file/how_to_deal_with_missing_data_in_supervised_deep_learning_.pdf
